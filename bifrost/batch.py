"""ë°°ì¹˜ ë¡œê·¸ ë¶„ì„"""

import asyncio
import aiofiles
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import time

from bifrost.ollama import OllamaClient
from bifrost.bedrock import BedrockClient, is_bedrock_available
from bifrost.preprocessor import LogPreprocessor
from bifrost.database import get_database
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn


class BatchAnalyzer:
    """ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°"""
    
    def __init__(
        self,
        source: str = "local",
        model: Optional[str] = None,
        max_workers: int = 4,
        use_cache: bool = True,
    ):
        self.source = source
        self.model = model
        self.max_workers = max_workers
        self.use_cache = use_cache
        self.console = Console()
        self.db = get_database()
        self.preprocessor = LogPreprocessor()
    
    async def analyze_files(self, file_paths: List[Path]) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ë¶„ì„"""
        results = []
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console,
        ) as progress:
            task = progress.add_task(f"[cyan]ë¶„ì„ ì¤‘...", total=len(file_paths))
            
            # ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            for file_path in file_paths:
                tasks.append(self._analyze_file(file_path))
            
            # ë™ì‹œ ì‹¤í–‰
            for coro in asyncio.as_completed(tasks):
                result = await coro
                results.append(result)
                progress.advance(task)
        
        return results
    
    async def _analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        start_time = time.time()
        
        try:
            # íŒŒì¼ ì½ê¸° (ë¹„ë™ê¸°)
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                log_content = await f.read()
            
            # ìºì‹œ í™•ì¸
            if self.use_cache:
                import hashlib
                log_hash = hashlib.sha256(log_content.encode()).hexdigest()
                cached = self.db.get_duplicate_analyses(log_hash, hours=24)
                
                if cached:
                    return {
                        "file": str(file_path),
                        "status": "cached",
                        "analysis_id": cached[0]["id"],
                        "response": cached[0]["response"],
                        "duration": time.time() - start_time,
                        "cached": True,
                    }
            
            # ì „ì²˜ë¦¬
            log_content = self.preprocessor.process(log_content)
            
            # í”„ë¡¬í”„íŠ¸
            from bifrost.main import MASTER_PROMPT
            prompt = MASTER_PROMPT.format(log_content=log_content)
            
            # ë¶„ì„ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=1) as executor:
                if self.source == "local":
                    client = OllamaClient(model=self.model or "mistral")
                    result = await loop.run_in_executor(
                        executor,
                        lambda: client.analyze(prompt, stream=False)
                    )
                elif self.source == "cloud":
                    if not is_bedrock_available():
                        raise Exception("Bedrock not available")
                    client = BedrockClient(model_id=self.model or "anthropic.claude-3-sonnet-20240229-v1:0")
                    result = await loop.run_in_executor(
                        executor,
                        lambda: client.analyze(prompt)
                    )
                else:
                    raise ValueError(f"Invalid source: {self.source}")
            
            duration = time.time() - start_time
            
            # DB ì €ì¥
            analysis_id = self.db.save_analysis(
                source=self.source,
                model=result["metadata"]["model"],
                log_content=log_content,
                response=result["response"],
                duration=duration,
                tags=[file_path.stem],
                service_name=file_path.stem,
                status="completed",
            )
            
            return {
                "file": str(file_path),
                "status": "success",
                "analysis_id": analysis_id,
                "response": result["response"],
                "duration": duration,
                "cached": False,
            }
        
        except Exception as e:
            return {
                "file": str(file_path),
                "status": "failed",
                "error": str(e),
                "duration": time.time() - start_time,
            }
    
    def analyze_directory(self, directory: Path, pattern: str = "*.log") -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ ë‚´ ë¡œê·¸ íŒŒì¼ ì¼ê´„ ë¶„ì„"""
        log_files = list(directory.glob(pattern))
        
        if not log_files:
            self.console.print(f"[yellow]âš ï¸  {directory}ì—ì„œ {pattern} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return []
        
        self.console.print(f"[cyan]ğŸ“ {len(log_files)}ê°œ íŒŒì¼ ë°œê²¬[/cyan]")
        
        # asyncio ì‹¤í–‰
        return asyncio.run(self.analyze_files(log_files))


async def analyze_stream(log_stream: asyncio.Queue, source: str = "local", model: Optional[str] = None):
    """ìŠ¤íŠ¸ë¦¼ ë¡œê·¸ ë¶„ì„ (ì‹¤ì‹œê°„)"""
    db = get_database()
    preprocessor = LogPreprocessor()
    
    if source == "local":
        client = OllamaClient(model=model or "mistral")
    elif source == "cloud":
        client = BedrockClient(model_id=model or "anthropic.claude-3-sonnet-20240229-v1:0")
    else:
        raise ValueError(f"Invalid source: {source}")
    
    while True:
        # íì—ì„œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
        log_content = await log_stream.get()
        
        if log_content is None:  # ì¢…ë£Œ ì‹ í˜¸
            break
        
        try:
            # ì „ì²˜ë¦¬
            log_content = preprocessor.process(log_content)
            
            from bifrost.main import MASTER_PROMPT
            prompt = MASTER_PROMPT.format(log_content=log_content)
            
            # ë¶„ì„
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=1) as executor:
                result = await loop.run_in_executor(
                    executor,
                    lambda: client.analyze(prompt, stream=False)
                )
            
            # DB ì €ì¥
            db.save_analysis(
                source=source,
                model=result["metadata"]["model"],
                log_content=log_content,
                response=result["response"],
                duration=result["metadata"]["duration"],
                status="completed",
            )
            
            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(log_content)} bytes")
        
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        finally:
            log_stream.task_done()
