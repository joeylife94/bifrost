"""FastAPI REST API ì„œë²„"""

import time
from typing import Optional, List
from datetime import datetime

from fastapi import FastAPI, HTTPException, Depends, Header, BackgroundTasks, WebSocket, WebSocketDisconnect, Request, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field

from bifrost.ollama import OllamaClient
from bifrost.bedrock import BedrockClient, is_bedrock_available
from bifrost.database import get_database, Database
from bifrost.preprocessor import LogPreprocessor
from bifrost.metrics import PrometheusMetrics
from bifrost.logger import logger
from bifrost.ratelimit import RateLimiter
from bifrost.exceptions import BifrostException, RateLimitError, handle_exception
from bifrost.validators import InputValidator
from bifrost.filters import LogFilter, SeverityLevel
from bifrost.export import DataExporter
from bifrost.slack import SlackNotifier

# FastAPI ì•±
app = FastAPI(
    title="Bifrost API",
    description="ğŸŒˆ The Rainbow Bridge for MLOps - AI-powered log analysis",
    version="0.2.0",
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ í•„ìš”
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Prometheus ë©”íŠ¸ë¦­
metrics = PrometheusMetrics()

# Rate Limiter ì´ˆê¸°í™”
rate_limiter = RateLimiter(requests_per_hour=100)

# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(BifrostException)
async def bifrost_exception_handler(request: Request, exc: BifrostException):
    logger.error(
        f"Bifrost error: {exc.message}",
        error_code=exc.code,
        path=str(request.url.path) if hasattr(request.url, 'path') else str(request.url)
    )
    return JSONResponse(
        status_code=400,
        content=handle_exception(exc)
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    logger.error(
        f"Unhandled error: {str(exc)}",
        path=str(request.url.path) if hasattr(request.url, 'path') else str(request.url),
        exception_type=type(exc).__name__
    )
    return JSONResponse(
        status_code=500,
        content=handle_exception(exc)
    )


# ==================== Pydantic Models ====================

class AnalyzeRequest(BaseModel):
    """ë¶„ì„ ìš”ì²­"""
    log_content: str = Field(..., description="ë¡œê·¸ ë‚´ìš©")
    source: str = Field("local", description="ë¶„ì„ ì†ŒìŠ¤ (local/cloud)")
    model: Optional[str] = Field(None, description="ëª¨ë¸ëª…")
    service_name: Optional[str] = Field(None, description="ì„œë¹„ìŠ¤ ì´ë¦„")
    environment: Optional[str] = Field(None, description="í™˜ê²½ (prod/staging)")
    tags: Optional[List[str]] = Field(default_factory=list, description="íƒœê·¸")
    stream: bool = Field(False, description="ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ")


class AnalyzeResponse(BaseModel):
    """ë¶„ì„ ì‘ë‹µ"""
    id: int
    response: str
    duration_seconds: float
    model: str
    cached: bool = False


class HistoryQuery(BaseModel):
    """íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    limit: int = Field(50, ge=1, le=500)
    offset: int = Field(0, ge=0)
    service_name: Optional[str] = None
    model: Optional[str] = None
    status: Optional[str] = None


class MetricsResponse(BaseModel):
    """ë©”íŠ¸ë¦­ ì‘ë‹µ"""
    total_analyses: int
    avg_duration_seconds: float
    model_stats: List[dict]


class FilterSeverityRequest(BaseModel):
    """ì‹¬ê°ë„ í•„í„°ë§ ìš”ì²­"""
    log_content: str = Field(..., description="ë¡œê·¸ ë‚´ìš©")
    min_level: SeverityLevel = Field(SeverityLevel.INFO, description="ìµœì†Œ ì‹¬ê°ë„")


class FilterErrorsRequest(BaseModel):
    """ì—ëŸ¬ í•„í„°ë§ ìš”ì²­"""
    log_content: str = Field(..., description="ë¡œê·¸ ë‚´ìš©")


class FilterKeywordsRequest(BaseModel):
    """í‚¤ì›Œë“œ í•„í„°ë§ ìš”ì²­"""
    log_content: str = Field(..., description="ë¡œê·¸ ë‚´ìš©")
    keywords: List[str] = Field(..., description="í‚¤ì›Œë“œ ëª©ë¡")
    case_sensitive: bool = Field(False, description="ëŒ€ì†Œë¬¸ì êµ¬ë¶„")


class SlackNotificationRequest(BaseModel):
    """Slack ì•Œë¦¼ ìš”ì²­"""
    webhook_url: str = Field(..., description="Slack Webhook URL")
    result: dict = Field(..., description="ë¶„ì„ ê²°ê³¼")
    service_name: Optional[str] = Field(None, description="ì„œë¹„ìŠ¤ ì´ë¦„")


class CreatePromptRequest(BaseModel):
    """í”„ë¡¬í”„íŠ¸ ìƒì„± ìš”ì²­"""
    name: str = Field(..., description="í”„ë¡¬í”„íŠ¸ ì´ë¦„")
    content: str = Field(..., description="í”„ë¡¬í”„íŠ¸ ë‚´ìš©")
    description: Optional[str] = Field(None, description="í”„ë¡¬í”„íŠ¸ ì„¤ëª…")
    tags: Optional[List[str]] = Field(None, description="íƒœê·¸ ëª©ë¡")


class CompareMLflowRunsRequest(BaseModel):
    """MLflow Run ë¹„êµ ìš”ì²­"""
    run_ids: List[str] = Field(..., description="ë¹„êµí•  Run ID ë¦¬ìŠ¤íŠ¸")
    metric_names: Optional[List[str]] = Field(None, description="ë©”íŠ¸ë¦­ ì´ë¦„ ëª©ë¡")


# ==================== Dependencies ====================

async def verify_api_key(x_api_key: Optional[str] = Header(None)) -> bool:
    """API í‚¤ ê²€ì¦"""
    if not x_api_key:
        return True  # ê°œë°œ ëª¨ë“œ: API í‚¤ ì„ íƒ
    
    db = get_database()
    if not db.validate_api_key(x_api_key):
        raise HTTPException(status_code=401, detail="Invalid API key")
    return True


# ==================== Routes ====================

@app.get("/")
async def root():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "name": "Bifrost API",
        "version": "0.2.0",
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
    }


@app.get("/health")
async def health():
    """ìƒì„¸ í—¬ìŠ¤ ì²´í¬"""
    db = get_database()
    
    ollama_healthy = False
    try:
        client = OllamaClient()
        ollama_healthy = client.health_check()
    except:
        pass
    
    # Kafka ìƒíƒœ í™•ì¸
    kafka_status = "disabled"
    if kafka_consumer_manager or kafka_producer_manager:
        kafka_status = "enabled"
    
    return {
        "status": "healthy",
        "components": {
            "database": "ok",
            "ollama": "ok" if ollama_healthy else "unavailable",
            "bedrock": "ok" if is_bedrock_available() else "not_configured",
            "kafka": kafka_status,
            "heimdall_integration": "enabled" if kafka_consumer_manager else "disabled",
        },
        "timestamp": datetime.utcnow().isoformat(),
    }


@app.get("/api/v1/heimdall/status")
async def heimdall_integration_status(_: bool = Depends(verify_api_key)):
    """Heimdall ì—°ë™ ìƒíƒœ í™•ì¸"""
    from bifrost.config import Config
    config = Config()
    
    kafka_enabled = config.get("kafka.enabled", False)
    heimdall_enabled = config.get("heimdall.enabled", False)
    
    status = {
        "integration_enabled": kafka_enabled and heimdall_enabled,
        "kafka": {
            "enabled": kafka_enabled,
            "bootstrap_servers": config.get("kafka.bootstrap_servers"),
            "consumer_running": kafka_consumer_manager is not None,
            "producer_running": kafka_producer_manager is not None,
        },
        "heimdall": {
            "enabled": heimdall_enabled,
            "callback_topic": config.get("heimdall.callback_topic"),
            "ai_source": config.get("heimdall.ai_source"),
        },
        "topics": config.get("kafka.topics", {}),
    }
    
    return status


@app.post("/analyze", response_model=AnalyzeResponse, dependencies=[Depends(verify_api_key)])
async def analyze_log(request: AnalyzeRequest, background_tasks: BackgroundTasks):
    """ë¡œê·¸ ë¶„ì„ API"""
    start_time = time.time()
    db = get_database()
    
    # ìºì‹œ í™•ì¸ (ì¤‘ë³µ ë¶„ì„ ë°©ì§€)
    import hashlib
    log_hash = hashlib.sha256(request.log_content.encode()).hexdigest()
    cached_results = db.get_duplicate_analyses(log_hash, hours=24)
    
    if cached_results and not request.stream:
        cached = cached_results[0]
        metrics.increment_cache_hits()
        return AnalyzeResponse(
            id=cached["id"],
            response=cached["response"],
            duration_seconds=cached["duration_seconds"],
            model=cached["model"],
            cached=True,
        )
    
    # ì „ì²˜ë¦¬
    preprocessor = LogPreprocessor()
    log_content = preprocessor.process(request.log_content)
    
    # í”„ë¡¬í”„íŠ¸
    from bifrost.main import MASTER_PROMPT
    prompt = MASTER_PROMPT.format(log_content=log_content)
    
    try:
        # ë¶„ì„ ì‹¤í–‰
        if request.source == "local":
            client = OllamaClient(model=request.model or "mistral")
            result = client.analyze(prompt, stream=False)  # APIëŠ” ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›
        elif request.source == "cloud":
            if not is_bedrock_available():
                raise HTTPException(status_code=400, detail="Bedrock not available (boto3 not installed)")
            client = BedrockClient(model_id=request.model or "anthropic.claude-3-sonnet-20240229-v1:0")
            result = client.analyze(prompt)
        else:
            raise HTTPException(status_code=400, detail="Invalid source (local or cloud)")
        
        duration = time.time() - start_time
        
        # DB ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ)
        analysis_id = db.save_analysis(
            source=request.source,
            model=result["metadata"]["model"],
            log_content=request.log_content,
            response=result["response"],
            duration=duration,
            tags=request.tags,
            service_name=request.service_name,
            environment=request.environment,
            tokens_used=result["metadata"].get("usage", {}).get("total_tokens"),
            status="completed",
        )
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metrics.increment_analysis_count(request.source)
        metrics.observe_analysis_duration(duration, request.source)
        
        return AnalyzeResponse(
            id=analysis_id,
            response=result["response"],
            duration_seconds=round(duration, 2),
            model=result["metadata"]["model"],
            cached=False,
        )
    
    except Exception as e:
        # ì—ëŸ¬ ì €ì¥
        db.save_analysis(
            source=request.source,
            model=request.model or "unknown",
            log_content=request.log_content,
            response="",
            duration=time.time() - start_time,
            status="failed",
            error_message=str(e),
        )
        
        metrics.increment_error_count(request.source)
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


@app.post("/history", response_model=List[dict])
async def get_history(query: HistoryQuery, _: bool = Depends(verify_api_key)):
    """ë¶„ì„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    db = get_database()
    results = db.list_analyses(
        limit=query.limit,
        offset=query.offset,
        service_name=query.service_name,
        model=query.model,
        status=query.status,
    )
    return results


@app.get("/history/{analysis_id}", response_model=dict)
async def get_analysis_detail(analysis_id: int, _: bool = Depends(verify_api_key)):
    """íŠ¹ì • ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    db = get_database()
    result = db.get_analysis(analysis_id)
    if not result:
        raise HTTPException(status_code=404, detail="Analysis not found")
    return result


@app.get("/metrics", response_model=MetricsResponse)
async def get_metrics(hours: int = 24, _: bool = Depends(verify_api_key)):
    """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    db = get_database()
    summary = db.get_metrics_summary(hours=hours)
    return MetricsResponse(**summary)


@app.get("/metrics/prometheus")
async def get_prometheus_metrics():
    """Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸"""
    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
    from fastapi.responses import Response
    
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.websocket("/ws/analyze")
async def websocket_analyze(websocket: WebSocket):
    """WebSocket ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„"""
    await websocket.accept()
    
    try:
        while True:
            # ìš”ì²­ ë°›ê¸°
            data = await websocket.receive_json()
            log_content = data.get("log_content", "")
            source = data.get("source", "local")
            model = data.get("model")
            
            if not log_content:
                await websocket.send_json({"error": "log_content is required"})
                continue
            
            # ì „ì²˜ë¦¬
            preprocessor = LogPreprocessor()
            log_content = preprocessor.process(log_content)
            
            from bifrost.main import MASTER_PROMPT
            prompt = MASTER_PROMPT.format(log_content=log_content)
            
            # ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„
            if source == "local":
                client = OllamaClient(model=model or "mistral")
                # TODO: WebSocketìš© ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
                result = client.analyze(prompt, stream=False)
                await websocket.send_json({
                    "type": "complete",
                    "response": result["response"],
                    "metadata": result["metadata"],
                })
            else:
                await websocket.send_json({"error": "Cloud streaming not supported yet"})
    
    except WebSocketDisconnect:
        pass


@app.post("/api-keys", dependencies=[Depends(verify_api_key)])
async def create_api_key(name: str, rate_limit: int = 100, description: Optional[str] = None):
    """API í‚¤ ìƒì„± (ê´€ë¦¬ììš©)"""
    db = get_database()
    key = db.create_api_key(name=name, rate_limit=rate_limit, description=description)
    return {"key": key, "name": name, "rate_limit": rate_limit}


@app.get("/api-keys", dependencies=[Depends(verify_api_key)])
async def list_api_keys():
    """API í‚¤ ëª©ë¡"""
    db = get_database()
    return db.list_api_keys()


# ==================== Startup/Shutdown ====================

# Kafka í†µí•© ê´€ë ¨ ì „ì—­ ë³€ìˆ˜
kafka_consumer_manager = None
kafka_producer_manager = None

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ"""
    global kafka_consumer_manager, kafka_producer_manager
    
    # Database ì´ˆê¸°í™”
    db = get_database()
    db.init_db()
    print("ğŸŒˆ Bifrost API Server started!")
    
    # Kafka í†µí•© í™œì„±í™” (ì„¤ì • ê¸°ë°˜)
    from bifrost.config import Config
    config = Config()
    
    kafka_enabled = config.get("kafka.enabled", False)
    heimdall_enabled = config.get("heimdall.enabled", False)
    
    if kafka_enabled and heimdall_enabled:
        try:
            from bifrost.kafka_consumer import KafkaConsumerManager
            from bifrost.kafka_producer import KafkaProducerManager
            from bifrost.heimdall_integration import HeimdallIntegrationService
            
            # Producer ì´ˆê¸°í™”
            kafka_config = config.get("kafka", {})
            kafka_producer_manager = KafkaProducerManager(kafka_config)
            await kafka_producer_manager.start()
            
            # Integration Service ì´ˆê¸°í™”
            integration_service = HeimdallIntegrationService(
                config=config.data,
                producer_manager=kafka_producer_manager
            )
            
            # Consumer ì´ˆê¸°í™” ë° ì‹œì‘
            kafka_consumer_manager = KafkaConsumerManager(kafka_config)
            await kafka_consumer_manager.start(
                integration_service.process_analysis_request
            )
            
            logger.info(
                "Kafka integration enabled - Heimdall ì—°ë™ ì‹œì‘ë¨",
                bootstrap_servers=kafka_config.get("bootstrap_servers")
            )
            print("ğŸ”— Kafka integration with Heimdall enabled!")
            
        except Exception as e:
            logger.error(f"Failed to initialize Kafka integration: {e}", exc_info=True)
            print(f"âš ï¸  Kafka integration failed: {e}")
    else:
        print("â„¹ï¸  Kafka integration disabled (CLI mode)")


@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ"""
    global kafka_consumer_manager, kafka_producer_manager
    
    # Kafka ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    if kafka_consumer_manager:
        await kafka_consumer_manager.stop()
        print("ğŸ›‘ Kafka consumer stopped")
    
    if kafka_producer_manager:
        await kafka_producer_manager.stop()
        print("ğŸ›‘ Kafka producer stopped")
    
    print("ğŸ‘‹ Bifrost API Server shutting down...")


# ==================== ìƒˆë¡œìš´ ê¸°ëŠ¥ ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/", response_class=HTMLResponse)
async def web_ui():
    """ì›¹ UI (htmx)"""
    try:
        with open("static/index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "<h1>Web UI not found. Please check static/index.html</h1>"


@app.post("/api/analyze-web")
async def analyze_web(
    log_content: str = Form(...),
    source: str = Form("local"),
    severity: str = Form(None),
    service_name: str = Form(None),
    environment: str = Form(None),
):
    """ì›¹ UIìš© ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸ (Form ë°ì´í„°)"""
    try:
        # ì‹¬ê°ë„ í•„í„°ë§
        filtered_log = log_content
        if severity:
            filtered_log = LogFilter.filter_by_severity(
                log_content,
                min_level=SeverityLevel(severity)
            )
        
        # ë¶„ì„ ì‹¤í–‰
        preprocessor = LogPreprocessor()
        processed_log = preprocessor.process(filtered_log)
        
        if source == "local":
            client = OllamaClient()
        else:
            client = BedrockClient()
        
        result = client.analyze(processed_log)
        
        # DB ì €ì¥
        db = get_database()
        analysis_id = db.save_analysis(
            log_content=log_content,
            response=result.get("response", ""),
            source=source,
            service_name=service_name,
            environment=environment,
        )
        
        # HTML ì‘ë‹µ
        html = f"""
        <div class="result">
            <div class="alert alert-success">
                âœ… ë¶„ì„ ì™„ë£Œ! (ID: {analysis_id})
            </div>
            <h3>ğŸ“Š ë¶„ì„ ê²°ê³¼</h3>
            <pre>{result.get('response', 'No response')}</pre>
            
            <div class="stats">
                <div class="stat-card">
                    <div class="number">{result.get('model', 'N/A')}</div>
                    <div class="label">ëª¨ë¸</div>
                </div>
                <div class="stat-card">
                    <div class="number">{source}</div>
                    <div class="label">ì†ŒìŠ¤</div>
                </div>
                <div class="stat-card">
                    <div class="number">{service_name or 'N/A'}</div>
                    <div class="label">ì„œë¹„ìŠ¤</div>
                </div>
            </div>
        </div>
        """
        
        return HTMLResponse(content=html)
    
    except Exception as e:
        error_html = f"""
        <div class="result">
            <div class="alert alert-error">
                âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}
            </div>
        </div>
        """
        return HTMLResponse(content=error_html, status_code=400)


@app.get("/api/export/csv")
async def export_csv(
    limit: int = 100,
    api_key: str = Depends(verify_api_key)
):
    """ë¶„ì„ ê²°ê³¼ë¥¼ CSVë¡œ export"""
    db = get_database()
    results = db.get_analysis_history(limit=limit)
    
    csv_content = DataExporter.to_csv(results)
    
    return StreamingResponse(
        iter([csv_content]),
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename=bifrost_export_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.csv"
        }
    )


@app.get("/api/export/json")
async def export_json(
    limit: int = 100,
    pretty: bool = True,
    api_key: str = Depends(verify_api_key)
):
    """ë¶„ì„ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ export"""
    db = get_database()
    results = db.get_analysis_history(limit=limit)
    
    json_content = DataExporter.to_json(results, pretty=pretty)
    
    return StreamingResponse(
        iter([json_content]),
        media_type="application/json",
        headers={
            "Content-Disposition": f"attachment; filename=bifrost_export_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json"
        }
    )


@app.post("/api/filter/severity")
async def filter_by_severity(
    request: FilterSeverityRequest,
    api_key: str = Depends(verify_api_key)
):
    """ì‹¬ê°ë„ë¡œ í•„í„°ë§"""
    filtered = LogFilter.filter_by_severity(request.log_content, request.min_level)
    stats = LogFilter.get_log_statistics(filtered)
    
    return {
        "filtered_log": filtered,
        "statistics": stats
    }


@app.post("/api/filter/errors")
async def filter_errors_only(
    request: FilterErrorsRequest,
    api_key: str = Depends(verify_api_key)
):
    """ì—ëŸ¬ë§Œ ì¶”ì¶œ"""
    filtered = LogFilter.extract_errors_only(request.log_content)
    
    return {
        "filtered_log": filtered,
        "line_count": len(filtered.split('\n'))
    }


@app.post("/api/slack/send")
async def send_to_slack(
    request: SlackNotificationRequest,
    api_key: str = Depends(verify_api_key)
):
    """ë¶„ì„ ê²°ê³¼ë¥¼ Slackìœ¼ë¡œ ì „ì†¡"""
    slack = SlackNotifier(request.webhook_url)
    success = slack.send_analysis_result(request.result, request.service_name)
    
    return {
        "success": success,
        "message": "Slack ì „ì†¡ ì„±ê³µ" if success else "Slack ì „ì†¡ ì‹¤íŒ¨"
    }


@app.get("/api/log/stats")
async def get_log_statistics(
    log_content: str,
    api_key: str = Depends(verify_api_key)
):
    """ë¡œê·¸ í†µê³„"""
    stats = LogFilter.get_log_statistics(log_content)
    
    return stats


# ==================== í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ ====================

@app.post("/api/prompts")
async def create_prompt(
    request: CreatePromptRequest,
    api_key: str = Depends(verify_api_key)
):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
    from bifrost.prompt_editor import PromptEditor
    
    editor = PromptEditor()
    prompt_id = editor.create_prompt(
        name=request.name,
        content=request.content,
        description=request.description,
        tags=request.tags or []
    )
    
    return {
        "prompt_id": prompt_id,
        "message": f"Prompt '{request.name}' created successfully"
    }


@app.get("/api/prompts")
async def list_prompts(
    tags: Optional[str] = None,
    search: Optional[str] = None,
    limit: int = 50,
    api_key: str = Depends(verify_api_key)
):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¦¬ìŠ¤íŠ¸"""
    from bifrost.prompt_editor import PromptEditor
    
    editor = PromptEditor()
    tag_list = tags.split(',') if tags else None
    prompts = editor.list_prompts(tags=tag_list, search=search, limit=limit)
    
    return {
        "prompts": prompts,
        "count": len(prompts)
    }


@app.get("/api/prompts/{prompt_id}")
async def get_prompt(
    prompt_id: int,
    api_key: str = Depends(verify_api_key)
):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¡°íšŒ"""
    from bifrost.prompt_editor import PromptEditor
    
    editor = PromptEditor()
    prompt = editor.get_prompt(prompt_id)
    
    if not prompt:
        raise HTTPException(status_code=404, detail="Prompt not found")
    
    return prompt


@app.put("/api/prompts/{prompt_id}")
async def update_prompt(
    prompt_id: int,
    content: Optional[str] = None,
    description: Optional[str] = None,
    tags: Optional[List[str]] = None,
    api_key: str = Depends(verify_api_key)
):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì—…ë°ì´íŠ¸"""
    from bifrost.prompt_editor import PromptEditor
    
    editor = PromptEditor()
    success = editor.update_prompt(
        prompt_id=prompt_id,
        content=content,
        description=description,
        tags=tags
    )
    
    if not success:
        raise HTTPException(status_code=404, detail="Prompt not found")
    
    return {"message": "Prompt updated successfully"}


@app.delete("/api/prompts/{prompt_id}")
async def delete_prompt(
    prompt_id: int,
    api_key: str = Depends(verify_api_key)
):
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚­ì œ"""
    from bifrost.prompt_editor import PromptEditor
    
    editor = PromptEditor()
    success = editor.delete_prompt(prompt_id)
    
    if not success:
        raise HTTPException(status_code=404, detail="Prompt not found")
    
    return {"message": "Prompt deleted successfully"}


# ==================== MLflow ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/api/mlflow/experiments")
async def get_mlflow_experiments(
    api_key: str = Depends(verify_api_key)
):
    """MLflow ì‹¤í—˜ ì •ë³´ ì¡°íšŒ"""
    from bifrost.mlflow_tracker import MLflowTracker
    
    tracker = MLflowTracker()
    if not tracker.enabled:
        raise HTTPException(
            status_code=503,
            detail="MLflow not available. Install with: pip install mlflow"
        )
    
    experiment = tracker.get_experiment_info()
    return experiment or {}


@app.get("/api/mlflow/runs")
async def search_mlflow_runs(
    filter: Optional[str] = None,
    max_results: int = 100,
    api_key: str = Depends(verify_api_key)
):
    """MLflow Run ê²€ìƒ‰"""
    from bifrost.mlflow_tracker import MLflowTracker
    
    tracker = MLflowTracker()
    if not tracker.enabled:
        raise HTTPException(status_code=503, detail="MLflow not available")
    
    runs = tracker.search_runs(
        filter_string=filter,
        max_results=max_results
    )
    
    return {
        "runs": runs,
        "count": len(runs)
    }


@app.get("/api/mlflow/runs/{run_id}")
async def get_mlflow_run(
    run_id: str,
    api_key: str = Depends(verify_api_key)
):
    """MLflow Run ìƒì„¸ ì¡°íšŒ"""
    from bifrost.mlflow_tracker import MLflowTracker
    
    tracker = MLflowTracker()
    if not tracker.enabled:
        raise HTTPException(status_code=503, detail="MLflow not available")
    
    run = tracker.get_run(run_id)
    
    if not run:
        raise HTTPException(status_code=404, detail="Run not found")
    
    return run


@app.post("/api/mlflow/runs/compare")
async def compare_mlflow_runs(
    request: CompareMLflowRunsRequest,
    api_key: str = Depends(verify_api_key)
):
    """MLflow Run ë¹„êµ"""
    from bifrost.mlflow_tracker import MLflowTracker
    
    tracker = MLflowTracker()
    if not tracker.enabled:
        raise HTTPException(status_code=503, detail="MLflow not available")
    
    comparison = tracker.compare_runs(request.run_ids, request.metric_names)
    
    return comparison


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
